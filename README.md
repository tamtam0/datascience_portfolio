**DATA SCIENCE PORTFOLIO**

**TAMILSELVAN TAMILMANI**


**1	INTRODUCTION**

The Masters in Applied Data Science Program at Syracuse University offered me an opportunity to learn the end-to-end aspects of the data science practice. It helped me identify the business problem, develop strategies for the problems, collect and organize the data, analyze the data systematically by doing EDA, statical analysis, and data mining.

It also helped me effectively communicate the findings, both good and bad. Introduced the tools necessary to implement the business decisions. We will discuss in detail the learnings I had the opportunity to complete.

**2	INTRODUCTION TO DATA SCIENCE | PROF. GARY KRUDYS**


The main objective of this course is to perform data processing and modeling using R. And determine appropriate techniques for analyzing data.

Professor Krudy’s hands-on approach to R made language learning easy. He showed us the power of the ggplot package, which became my favorite plotting library. Below are the topics covered during the course

•	Data Collection & Sampling
•	Data Exploration – EDA - ggplot
•	Geospatial Data Visualization - ggplot
•	Linear Regression
•	Data Mining
o	Association Rule Mining
o	Support Vector Machines
•	Text Mining
o	EDA- Word Clouds
o	Sentiment Analysis
o	Topic Modeling


We utilized the data processing steps learned in the course to clean up the survey data and applied different machine learning techniques like SVM and linear regression on our final project. We uncovered more insights using association rules for this analysis, which gave us an idea of what real-world problems will look like. Sophisticated algorithms might not matter, just a systematic analysis to uncover the insights.



**3	BUSINESS ANALYTICS | PROF. KIMBERLY PENDELBERRY**

The course's main objective is to do data analysis and mine data, develop a strategy based on the data, and make decisions. This covered all three aspects of analytics: descriptive, predictive, and prescriptive.

Professor Pendelberry patiently answered all my questions. She introduced the basics of descriptive analytics, predictive model importance like R square, pitfalls like multicollinearity, serial correlation, and the prescriptive side of modeling using techniques like sensitivity analysis. Showed us the difference between linear, logistic, and neural networks. The in-class discussion about the MIT Sloan Management Review articles showed us a glimpse of how organizations are using analytics. Below are the topics covered during the course

•	Google Analytics
•	Tableau Dashboards
•	Optimization
•	Univariate & Multivariate Regression
•	One-Way & Two-way Sensitivity Analysis
•	Statistical analysis
o	Visualization – Histogram, Box, Scatter, Mean, XY Plots
o	Statistical Summaries, Correlation
o	ANNOVA
o	Regression
	Dummy Variables
	Moderating Effects
•	Regression Diagnostics
o	Linearity Test
o	Collinearity Test
o	Heteroscedasticity Test
o	Serial Correlation Test
o	Outlier Test
•	Bedford’s Law
•	Decision Trees
•	Choice Models
o	Logit & Probit Models
•	Perceptron & Neural Networks



We get to practice descriptive analytics using google analytics with actual Whiteman site data. Predicted book prices and recommended actions for bank loan data using prescriptive analytics.

**4	DATA ANALYTICS | PROF. JEREMY BOLTON**

The objective of the course is twofold: one is applying data mining techniques to the real-world data set, the other is to effectively tell a story about the analysis in a valuable way.

Professor Bolton set up a high bar on the storytelling part, specifically making sure we submitted all our reports in a professional format. Below are the topics covered during the course

•	Data Preparation - Summary Statistics, Aggregation, Transformation
•	Classification
o	Decision Tree Analysis
	C4.5
	Information gain, overfitting & pruning
o	Naïve Bayes
	Smoothing – Laplacian smoothing(add one)
o	SVM
	Soft & Max Margin
	Kernels – Polynomial
o	Random Forest
o	Ensemble Learning
•	Clustering Analysis
o	K – Means, Tuning & Centroid Interpretation
o	HAC Algorithm - Hierarchical Agglomerative Clustering
o	kNN
•	Association Rule Mining
•	Text Mining
o	Tokenization
o	Vectorization


In the Federalists Paper homework, we applied all forms of clustering like k-means, hierarchical, and EM(Expectation-Maximization) and different distance measures like Manhattan, Euclidian, and Minkowski.

In the Handwritten recognition homework, we had changed to explore classification methods like a decision tree and naïve Bayes. We got introduced to feature selection/reduction methods like PCA.

We explored a time series data set on our final project and applied all the data mining techniques we learned. We started from association rule mining, k-means, KNN, SVM, decision tree, and random forest. We have tried new feature engineering techniques like lagged features.

**5	TEXT MINING | PROF. AMY GATES**

This course aims to learn the basics of text mining like document representation, information extraction, text classification and clustering, and topic modeling.

Professor Gates is efficient in her approach, and she encouraged us to go beyond the curriculum and explore the state-of-the-art NLP models. And she set the highest bar in terms of report submission. Below are the topics covered during the course

•	Document Vectorization
o	Tokenization
o	Vectorization
•	Corpus Analytics
o	Regex
o	Lexicons
o	Comparative & Trend Analytics
•	Text Classification
o	Multinomial Naïve Bayes – feature ranking
o	Decision Tress
o	SVM
o	Evaluation Metrics
•	Scikit-learn
•	Document Clustering & Topic Modeling
o	K-Means
o	LDA
•	Bias in AI


We did sentiment analysis and topic modeling for the data from Twitter, Reddit, and news sites.

**6	DATA ANALYSIS & DECISION MAKING | PROF. KIMBERLY PENDELBERRY**

The objective of this course is to select the appropriate statistical technique for a given set of conditions to answer a particular question.

We explored hypothesis testing for continuous variables chi-square test for discrete variables. Get to try out the kappa agreement score using Amazon Mechanical Truck for sentiment data. We got an introduction to time series data and process controls from Professor Pendelberry. Below are the topics covered during the course

•	DMAIC (Define, Measure, Analyze, Improve, and Control)
•	Sigma Quality Levels
•	Kappa Technique
•	Probability Distributions & Hypothesis Testing
o	Normal & Binomial distribution
o	H0 & Ha Scenarios
o	Alpha & Beta risks
•	Categorical Data Analysis
o	Chi-Square Test
•	Confidence Intervals
•	Simple & Multiple Linear Regression
•	Process Control Charts
o	X-bar/R, ImR
•	Timeseries Analysis
o	Autocorrelation
o	Auto Regressive Models, AR1
o	Moving Average Models
o	Exponential Smoothing Models

The final process improvement project allowed me to define the problem using my data.


**7	MARKETING ANALYTICS | PROF. RAGHAVSHYAM RAMAMURTHY**

The objective of this course is to develop marketing strategies and resource allocation decisions driven by quantitative analysis

Professor Ramamurthy took us through the complete journey with marketing analytics starting with Customer Segmentation, Marketing mix models, Choice, and Churn models, and Conjoint analysis. We are exposed to all these models already, but the practical application of the models to business problems proved to be very tough. Below are the topics covered during the course

•	Marketing Resource Allocation
o	Opportunity
o	Resource Allocation
o	Optimizing Sales
o	ROI
•	Cluster Analysis
o	Segmentation using k-means
o	Profiling Segmentation
•	Regression
o	Omitted Variable Bias
o	Price Elasticity
o	Statistical vs Economic significance
•	Customer Lifetime Value
o	Base CLV
o	Cohort
•	Propensity models
o	Market Share Prediction
o	Hit Rates
o	Association Rule Mining
•	Churn Prediction
o	pAlive Calculation
o	Logistic Regression
•	Choice Models
o	Utility Theory
o	Conjoint Analysis
o	Cross-Selling
•	Marketing Experiments
o	Marketing-Mix models
o	Test & Control groups
•	Collaborative Filtering


Our final project gave us a chance to explore all the marketing analytics techniques we learned, and the simulation gave us an idea of how hard it is to run a multi-year marketing campaign.

**8	BIG DATA ANALYTICS | PROF. JONATHAN FOX**

The objective of the course is to complete the entire lifecycle of data science, from collecting data to delivering insights.

Professor Fox made the class-competitive and fun. I created the highest number of models in this class. He took us through the full OSEMIN (Obtain, Scrub, Explore, Model & Interpret) cycle. Below are the topics covered during the course

•	OSEMiN
•	Python & Spark
•	Data Review
•	Model Review
•	Code Review
•	Consumer preference
•	Evaluation Methods
•	Forecasting – ARMIA
•	Poisson & Negative Binomial Distributions
•	Decision Trees & Random Forest
•	Neural Networks
•	Convolution Neural Networks
•	Presentation
o	Elders Rule


Each homework assignment pushed the limits on what we could do. The Football dataset homework made us crawl the web to augment external data. The time-series Zillow dataset made us run 30 thousand models, and finally, the Fashion MINIST data set made us compare all the classical approaches against Neural Networks.

The Final Project, “COVID Detection from X-Ray Images,” gave us a chance to produce meaningful work and showcase the power of transfer learning in neural networks. 


**9	INFORMATION VISUALIZATION | PROF. RAGHAVSHYAM RAMAMURTHY**

The objective of the course is to learn and practice advanced visual storytelling.

Professor Ramamurthy is hands-on in transforming the basic SVG plots created in R to beautiful data stories in Adobe Illustrator. Below are the topics covered during the course

•	Graphic Design Principles
o	Colors
o	Layouts
o	Element Design
•	ggplot
•	Social Network Analysis & Graphs


The final Netflix analytics Project allowed me to convey my data story in a visually appealing way.


**10	NATURAL LANGUAGE PROCESSING | PROF. MICHAEL LARCHE**

The objective of the course is to understand the levels of linguistic analysis and apply them using the NLTK library

Professor Larche made the class interactive by using the async assignments in class and asking questions to explain the concepts. He methodically introduced regex, Parts of Speech tagging, context-free grammar, sentiment analysis and finally ended in deep learning. Below are the topics covered during the course

•	Corpus Statistics
o	Unigrams & Bigrams
o	Mutual Information
•	Language Modeling
•	Morphology
•	Parts-of-Speech Tagging
•	Context-Free-Grammar
•	Semantics
•	Discourse & Dialogue
•	NLTK
•	Sentiment Analysis & Classification


The final project was structured in a way we get to use the classical NLP methods along with deep learning. Although we learned the classical NLP, working on deep learning made us question the need for the classical approaches.

**11	QUANTITATIVE REASONING | PROF. GREGORY BLOCK**

The objective of the course is to advance our understanding of the inference of the data by comparing Frequentist and Bayesian methods.

Professor Block's had a hands-on approach in explaining the frequentist and the corresponding Bayesian equivalents like High-Density Interval, Markov chain - Monte Carlo methods, and Bayes Factor. Below are the topics covered during the course

•	Population & Samples
•	Law of large Numbers
•	Frequentist vs Bayesian approaches on
o	ANOVA
o	Cross-Products/Pearson Product-Moment Correlation
o	Chi-Square Distribution
•	Linear Multiple Regression
o	Least Square Criterion, Sum of Squared Errors
o	Multicollinearity
o	Bayesian Inference
•	Logistic Regression
o	GLM & Link Functions
o	Bayesian Estimates
•	Timeseries
o	Changepoint Analysis
o	Repeated Measure ANOVA
o	Trend Removal
o	Diagnosing and Testing Stationarity
o	ARIMA


The Final exam allowed us to apply all the Bayesian techniques and compare them with the corresponding frequentist methods.


**12	CONCLUSION**

The program increased my knowledge and confidence in the data science practice. Introduced me to unique tools to help me understand the data communicate effectively. Changed my perspective on how to approach the business problem.

All the work I have done in the program is consolidated in the below GitHub repository. The repository is organized by the order I took the course. The individual work is organized as subfolders, and the contents are self-explanatory.

https://github.com/tamtam0/datascience_portfolio 
