{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('movie_review_one.csv')\n",
    "texts = df['Phrase'].str.lower().to_list()\n",
    "labels = df['Sentiment'].astype(str).to_list()\n",
    "test_train = df['label'].to_list()\n",
    "word_tokens = texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def print_eval_measures(gold, predicted):\n",
    "    # get a list of labels\n",
    "    labels = [\"0\",\"1\",\"2\",\"3\",\"4\"]\n",
    "    # these lists have values for each label\n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        print(\"Processing label :{}\".format(lab))\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)  if(TP!=0 and FP!=0) else 0\n",
    "        precision = TP / (TP + FN) if(TP!=0 and FN!=0) else 0\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        if( recall != 0 and precision !=0):\n",
    "            F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "        else:\n",
    "            F1_list.append(0)\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]),\n",
    "              \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "\n",
    "    cm = nltk.ConfusionMatrix(gold, predicted)\n",
    "    #print(cm.pretty_format(sort_by_count=False, truncate=9))\n",
    "    # or show the results as percentages\n",
    "    print(cm.pretty_format(sort_by_count=False,values_in_chart=True, show_percents=True, truncate=9))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def run_nb(featuresets, test_train):\n",
    "    train = []\n",
    "    test = []\n",
    "    for t,f in  zip(test_train,featuresets):\n",
    "        if(t == \"train\"):\n",
    "            train.append(f)\n",
    "        else:\n",
    "            test.append(f)\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "    predicted=classifier.classify_many([fs for (fs, l) in test])\n",
    "    gold =[]\n",
    "    for f,l in test:\n",
    "        gold.append(l)\n",
    "    print(\"length predicted:{}, gold:{}\".format(len(predicted),len(gold)))\n",
    "    print_eval_measures(gold,predicted)\n",
    "\n",
    "    correct = [l == r for  l, r in zip(gold, predicted)]\n",
    "    accuracy = sum(correct) / len(correct)\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    #print(\"Top 30 Features\")\n",
    "    #classifier.show_most_informative_features(30)\n",
    "    return  classifier\n",
    "    # evaluate the accuracy of the classifier\n",
    "    #accuracy=nltk.classify.accuracy(classifier, test)\n",
    "    #print(\"Accuracy: {}\".format(accuracy))\n",
    "\n",
    "    # the accuracy result may vary since we randomized the documents\n",
    "\n",
    "    # show which features of classifier are most informative\n",
    "def explain_model(classifier):\n",
    "    classifier.show_most_informative_features(30)\n",
    "\n",
    "#sen_tokens = [nltk.sent_tokenize(text) for text in texts]\n",
    "#word_tokens = [nltk.word_tokenize(sent) for sent in sen_tokens]\n",
    "\n",
    "#nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "#morestopwords = ['ii','eh',\"'\",'?','*',\"'ye\",'ye','us','could','would','might','must','need','sha','wo','y',\"'s\",\"'d\",\"'ll\",\"'t\",\"'m\",\"'re\",\"'ve\", \"n't\"]\n",
    "\n",
    "#stopwords = nltkstopwords + morestopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def run_cross_validation_nb(num_folds, featuresets):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    print('Folds: {} , Each fold size:{}'.format(num_folds,subset_size))\n",
    "    accuracy_list = []\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[(i*subset_size):][:subset_size]\n",
    "        train_this_round = featuresets[:(i*subset_size)] + featuresets[((i+1)*subset_size):]\n",
    "        # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        predicted=classifier.classify_many([fs for (fs, l) in test_this_round])\n",
    "        gold =[]\n",
    "        for f,l in test_this_round:\n",
    "            gold.append(l)\n",
    "        print_eval_measures(predicted,gold)\n",
    "\n",
    "        correct = [l == r for  l, r in zip(gold, predicted)]\n",
    "        accuracy_this_round = sum(correct) / len(correct)\n",
    "        print(\"Accuracy: {}\".format(accuracy_this_round))\n",
    "        print (i, accuracy_this_round)\n",
    "        accuracy_list.append(accuracy_this_round)\n",
    "    # find mean accuracy over all rounds\n",
    "    print ('mean accuracy', sum(accuracy_list) / num_folds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'c1': 'p', 'c2': 'r', 'c3': 'o', 'l1': 'd', 'l2': 'e', 'l3': 's', 'l4': 's'}, '2')\n",
      "length predicted:4960, gold:4960\n",
      "Processing label :0\n",
      "Processing label :1\n",
      "Processing label :2\n",
      "Processing label :3\n",
      "Processing label :4\n",
      "\tPrecision\tRecall\t\tF1\n",
      "0 \t      0.000      0.000      0.000\n",
      "1 \t      0.064      0.319      0.107\n",
      "2 \t      0.966      0.748      0.843\n",
      "3 \t      0.010      0.158      0.020\n",
      "4 \t      0.030      0.080      0.044\n",
      "  |      0      1      2      3      4 |\n",
      "--+------------------------------------+\n",
      "0 |     <.>  0.1%   1.1%      .   0.0% |\n",
      "1 |   0.0%  <0.7%> 10.7%   0.1%      . |\n",
      "2 |   0.4%   1.2% <71.8%>  0.5%   0.4% |\n",
      "3 |      .   0.2%  11.2%  <0.1%>  0.1% |\n",
      "4 |      .   0.1%   1.2%   0.0%  <0.0%>|\n",
      "--+------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Accuracy: 0.7268145161290323\n"
     ]
    }
   ],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "def sentiment_features(word):\n",
    "    #org_word=word\n",
    "    word=lancaster.stem(word)\n",
    "    length = len(word)\n",
    "    features = {\n",
    "                \"c1\":word[0],\n",
    "                \"c2\":word[1]  if(length>1) else \"\",\n",
    "                \"c3\":word[2]  if(length>2) else \"\" ,\n",
    "                #\"c4\":word[3]  if(length>3) else \"\" ,\n",
    "                #\"c5\":word[4]  if(length>4) else \"\" ,\n",
    "                #\"c6\":word[5]  if(length>5) else \"\" ,\n",
    "                #\"c7\":word[6]  if(length>6) else \"\" ,\n",
    "                #\"c8\":word[7]  if(length>7) else \"\" ,\n",
    "                #\"c9\":word[8]  if(length>8) else \"\" ,\n",
    "                #\"c10\":word[9]  if(length>9) else \"\" ,\n",
    "                #\"bi1\":word[0:1]  if(length>1) else \"\" ,\n",
    "                #\"bi2\":word[1:2]  if(length>1) else \"\" ,\n",
    "                #\"m0\":word[round(length/2)-1] if(length>7) else \"\",\n",
    "                #\"m1\":word[round(length/2)] if(length>7) else \"\",\n",
    "                #\"m2\":word[round(length/2)+1] if(length>7) else \"\",\n",
    "                \"l1\":word[-1],\n",
    "                \"l2\":word[-2]  if(length>2) else \"\",\n",
    "                \"l3\":word[-3]  if(length>3) else \"\",\n",
    "                \"l4\":word[-4]  if(length>4) else \"\",\n",
    "                #\"l5\":word[-5]  if(length>5) else \"\",\n",
    "                #\"bil1\":word[-1:],\n",
    "                #\"bil2\":word[length-2:length-1],\n",
    "                #\"pos\":nltk.pos_tag([word])[0][1],\n",
    "                #\"len\":length\n",
    "                }\n",
    "    # tag = nltk.pos_tag([org_word])[0][1]\n",
    "    # tag_type=\"\"\n",
    "    # if tag.startswith('N'): tag_type='Noun'\n",
    "    # if tag.startswith('V'): tag_type='Verb'\n",
    "    # if tag.startswith('J'): tag_type='Adj'\n",
    "    # if tag.startswith('R'): tag_type='Adverb'\n",
    "    # if tag.startswith('M'): tag_type='Modal'\n",
    "    # if tag.startswith('D'): tag_type='Determiner'\n",
    "    # if tag.startswith('TO'): tag_type='TO'\n",
    "    # if tag.startswith('W'): tag_type='Wh'\n",
    "    # if tag.startswith('UH'): tag_type='Interjection'\n",
    "    # if tag.startswith('CC'): tag_type='Corconjunction'\n",
    "    # if tag.startswith('CD'): tag_type='Card'\n",
    "    # features['tag_type'] = tag_type\n",
    "    # features['Noun'] = True if tag.startswith('N') else False\n",
    "    # features['Verb'] = True if tag.startswith('V') else False\n",
    "    # features['Adj'] = True if tag.startswith('J') else False\n",
    "    # features['Adverb'] = True if tag.startswith('R') else False\n",
    "    # features['Modal'] = True if tag.startswith('M') else False\n",
    "    # features['Determiner'] = True if tag.startswith('D') else False\n",
    "    # features['TO'] = True if tag.startswith('TO') else False\n",
    "    # features['Interjections'] = True if tag.startswith('UH') else False\n",
    "    # features['Wh'] = True if tag.startswith('W') else False\n",
    "    # features['Interjection'] = True if tag.startswith('UH') else False\n",
    "    # features['Corconjunction'] = True if tag.startswith('CC') else False\n",
    "    # features['CardNum'] = True if tag.startswith('CD') else False\n",
    "    return features\n",
    "\n",
    "\n",
    "# get features sets for a document, including keyword features and category feature\n",
    "featuresets_sen1 = [(sentiment_features(text), label) for text, label in zip(word_tokens, labels)]\n",
    "\n",
    "print(featuresets_sen1[0])\n",
    "nb_sen=run_nb(featuresets_sen1,test_train)\n",
    "#explain_model(nb_uni)\n",
    "# perform the cross-validation on the featuresets with word features and generate accuracy\n",
    "num_folds = 5\n",
    "\n",
    "#run_cross_validation_nb(num_folds, featuresets_sen1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length predicted:16531, gold:16531\n",
      "Processing label :0\n",
      "Processing label :1\n",
      "Processing label :2\n",
      "Processing label :3\n",
      "Processing label :4\n",
      "\tPrecision\tRecall\t\tF1\n",
      "0 \t      0.005      0.032      0.009\n",
      "1 \t      0.097      0.390      0.155\n",
      "2 \t      0.972      0.752      0.848\n",
      "3 \t      0.019      0.324      0.035\n",
      "4 \t      0.041      0.158      0.065\n",
      "  |      0      1      2      3      4 |\n",
      "--+------------------------------------+\n",
      "0 |  <0.0%>  0.1%   1.1%   0.0%   0.0% |\n",
      "1 |      .  <1.1%> 10.4%   0.1%   0.0% |\n",
      "2 |   0.2%   1.4% <72.2%>  0.4%   0.2% |\n",
      "3 |   0.0%   0.2%  11.1%  <0.2%>  0.1% |\n",
      "4 |      .   0.0%   1.2%   0.0%  <0.1%>|\n",
      "--+------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Accuracy: 0.7356481761538927\n"
     ]
    }
   ],
   "source": [
    "\n",
    "featuresets_sen_all = [(sentiment_features(text), label) for text, label in zip(word_tokens, labels)]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(featuresets_sen_all)\n",
    "predicted=classifier.classify_many([fs for (fs, l) in featuresets_sen_all])\n",
    "gold =[]\n",
    "for f,l in featuresets_sen_all:\n",
    "    gold.append(l)\n",
    "print(\"length predicted:{}, gold:{}\".format(len(predicted),len(gold)))\n",
    "print_eval_measures(gold,predicted)\n",
    "\n",
    "correct = [l == r for  l, r in zip(gold, predicted)]\n",
    "accuracy = sum(correct) / len(correct)\n",
    "print(\"Accuracy: {}\".format(accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing label :0\n",
      "Processing label :1\n",
      "Processing label :2\n",
      "Processing label :3\n",
      "Processing label :4\n",
      "\tPrecision\tRecall\t\tF1\n",
      "0 \t      0.003      0.185      0.006\n",
      "1 \t      0.026      0.232      0.046\n",
      "2 \t      0.943      0.517      0.667\n",
      "3 \t      0.072      0.320      0.118\n",
      "4 \t      0.004      0.165      0.007\n",
      "  |      0      1      2      3      4 |\n",
      "--+------------------------------------+\n",
      "0 |  <0.0%>  0.1%   4.2%   0.2%   0.0% |\n",
      "1 |   0.0%  <0.4%> 16.2%   0.8%   0.0% |\n",
      "2 |   0.0%   1.1% <48.1%>  1.7%   0.1% |\n",
      "3 |   0.0%   0.2%  19.3%  <1.5%>  0.0% |\n",
      "4 |      .   0.0%   5.3%   0.5%  <0.0%>|\n",
      "--+------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Accuracy: 0.5010028258543243\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "df_all = pd.read_csv('movie_review.csv')\n",
    "texts_all = df_all['Phrase'].str.lower().to_list()\n",
    "labels_all = df_all['Sentiment'].astype(str).to_list()\n",
    "word_tokens_all = [nltk.word_tokenize(t) for t in texts_all]\n",
    "featuresets_sen_all = [(sentiment_features(text), label) for text, label in zip(word_tokens, labels)]\n",
    "classifier_all = nltk.NaiveBayesClassifier.train(featuresets_sen_all)\n",
    "predicted=[]\n",
    "gold= []\n",
    "for word_token,label in zip(word_tokens_all,labels_all):\n",
    "    if not len(word_token)==0 :\n",
    "        predicted_text=classifier_all.classify_many([sentiment_features(w) for w in word_token])\n",
    "        predicted_avg = math.ceil(sum([int(word) for word in predicted_text]) / len(word_token))\n",
    "        predicted.append(str(predicted_avg))\n",
    "        gold.append(label)\n",
    "\n",
    "print_eval_measures(gold,predicted)\n",
    "\n",
    "correct = [l == r for  l, r in zip(gold, predicted)]\n",
    "accuracy = sum(correct) / len(correct)\n",
    "print(\"Accuracy: {}\".format(accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "featuresets_sen1 = [(sentiment_features(text), label) for text, label in zip(word_tokens, labels)]\n",
    "\n",
    "print(featuresets_sen1[0])\n",
    "nb_sen=run_nb(featuresets_sen1,test_train)\n",
    "#explain_model(nb_uni)\n",
    "# perform the cross-validation on the featuresets with word features and generate accuracy\n",
    "num_folds = 5\n",
    "\n",
    "#run_cross_validation_nb(num_folds, featuresets_sen1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}