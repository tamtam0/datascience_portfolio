mod_cols<-c("Airline_Status:Type_of_Travel","Airline_Status:Arrival_Delay_greater_5_Mins","Age:No._of_other_Loyalty_Cards","Airline_Status:Class","Departure_Delay_in_Minutes:Flight_time_in_minutes","Age:Percent_of_Flight_with_other_Airlines","Type_of_Travel:Scheduled_Departure_Hour")
formula_sig_mod<-paste("Satisfaction", paste(c(sig_cols,mod_cols), collapse=" + "), sep=" ~ ")
lm.sig_mod_model<- lm(formula = formula_sig_mod,lmData)
summary(lm.sig_mod_model)
vif(lm.sig_mod_model)
#Final
sig_cols_vif<-c("Airline_Status","Gender","Age","No_of_Flights_p.a.","Eating_and_Drinking_at_Airport","Class","Scheduled_Departure_Hour","Flight_time_in_minutes")
mod_cols_vif<-c("Airline_Status:Type_of_Travel","Airline_Status:Arrival_Delay_greater_5_Mins","Age:No._of_other_Loyalty_Cards","Departure_Delay_in_Minutes:Flight_time_in_minutes","Age:Percent_of_Flight_with_other_Airlines")
formula_sig_mod_vif<-paste("Satisfaction", paste(c(sig_cols_vif,mod_cols_vif), collapse=" + "), sep=" ~ ")
lm.sig_mod_vif_model<- lm(formula = formula_sig_mod_vif,lmData)
summary(lm.sig_mod_vif_model)
#collinearity test
vif(lm.sig_mod_vif_model)
#linearity test
resettest(lm.sig_mod_vif_model)
#residual test
bptest(lm.sig_mod_vif_model)
#serial correlation test
dwtest(lm.sig_mod_vif_model)
#outlier test
outlierTest(lm.sig_mod_vif_model)
outlier<-lmData[7104,]
plot(lm.sig_mod_vif_model)
sig_numeric_columns<-numeric_columns[numeric_columns %in% sig_cols_vif]
sig_categorical_columns<-categorical_columns[categorical_columns %in% sig_cols_vif]
randIndex <- sample(1:nrow(surveyDataRaw))
cutpoint<- floor(nrow(surveyDataRaw)*2/3)
getSignificantDataAsSparse<- function(data){
sigData<-data[,c("Satisfaction",sig_numeric_columns)]
sigData$Satisfaction<-ifelse(sigData$Satisfaction>3,TRUE,FALSE)
#nnData$Satisfaction<-as.factor(ifelse(nnData$Satisfaction>3,1,0))
#nnData<-convertToFactors(nnData)
for(c in sig_categorical_columns){
if(c %in% sig_categorical_columns){
binary<-to.dummy(surveyDataRaw[[c]],c)
sigData<- cbind(sigData,binary)
}
}
return(sigData)
}
#neuralnet
library(neuralnet)
nnData<-getSignificantDataAsSparse(surveyDataRaw)
nnData$Satisfaction<-ifelse(nnData$Satisfaction==TRUE,1,0)
nn_trainData<- nnData[randIndex[1:cutpoint],]
nn_testData <- nnData[randIndex[(cutpoint+1):nrow(nnData)],]
nn_trainDataMatrix<-as.matrix(nn_trainData)
nn_testDataMatrix<-as.matrix(nn_testData)
#nn_formula <-formula_sig_mod_vif<-paste("Satisfaction", paste(c(sig_cols_vif), collapse=" + "), sep=" ~ ")
nn_model<- neuralnet(formula=Satisfaction ~.,data = nn_trainDataMatrix,hidden = 10,lifesign = "minimal", linear.output = FALSE,threshold = 0.1)
plot(nn_model)
nn_predicted<-compute(nn_model,nn_testDataMatrix)
nn_testData$predicted<-nn_predicted$net.result
percent_nn<-sum(ifelse(round(nn_testData$predicted)==nn_testData$Satisfaction,1,0))/length(nn_testData$predicted)
percent_nn
#nn_formula <-formula_sig_mod_vif<-paste("Satisfaction", paste(c(sig_cols_vif), collapse=" + "), sep=" ~ ")
nn_model<- neuralnet(formula=Satisfaction ~.,data = nn_trainDataMatrix,hidden = 12,lifesign = "minimal", linear.output = FALSE,threshold = 0.1)
#Age Group 53-85
ggplot(surveyDataRaw[surveyDataRaw$Age>=80,],aes(y=Satisfaction,x=Age)) + geom_smooth() + geom_count()
#VIsualizations  based on Significant LM vars
#Airline Status
ggplot(surveyDataRaw,aes(Airline_Status)) + geom_histogram(stat="count")
ggplot(surveyDataRaw,aes(y=Satisfaction,x=Airline_Status)) + geom_boxplot()
ggplot(surveyDataRaw,aes(y=Satisfaction,x=Age)) + geom_smooth() + geom_count()
ggplot(surveyDataRaw,aes(y=Satisfaction,x=Age)) + geom_smooth(method="glm") + geom_count()
ggplot(surveyDataRaw,aes(y=Satisfaction,x=Age)) + geom_smooth(method="glm") + geom_count()
#Age Group 53-85
ggplot(surveyDataRaw[surveyDataRaw$Age>=80,],aes(y=Satisfaction,x=Age)) + geom_smooth() + geom_count()
ggplot(surveyDataRaw[surveyDataRaw$Age==80,],aes(Age)) + geom_histogram(stat="count")
ggplot(surveyDataRaw,aes(y=Satisfaction,x=Age))  + geom_count()
ggplot(surveyDataRaw,aes(Gender)) + geom_histogram(stat="count")
ggplot(surveyDataRaw,aes(y=Satisfaction,x=Gender)) + geom_boxplot()
ggplot(surveyDataRaw,aes(y=Satisfaction,x=No_of_Flights_p.a.)) + geom_smooth(method="glm") + geom_count()
#Age
#ggplot(surveyDataRaw,aes(Age)) + geom_histogram(stat="count",line="blue")
ggplot(surveyDataRaw,aes(y=Satisfaction,x=Age)) + geom_smooth(method="glm") + geom_count()
#Gender
ggplot(surveyDataRaw,aes(Gender)) + geom_histogram(stat="count")
ggplot(surveyDataRaw,aes(y=Satisfaction,x=Gender)) + geom_boxplot()
# The mean and lower quater is same for female , which means female tend to score as low
#No of flights
#ggplot(surveyDataRaw,aes(No_of_Flights_p.a.)) + geom_histogram(stat="count")
ggplot(surveyDataRaw,aes(y=Satisfaction,x=No_of_Flights_p.a.)) + geom_smooth(method="glm") + geom_count()
# the higher the number of flight the lower the score is
#Type_of_Travel
ggplot(surveyDataRaw,aes(Type_of_Travel)) + geom_histogram(stat="count")
ggplot(surveyDataRaw,aes(y=Satisfaction,x=Type_of_Travel)) + geom_boxplot()
#Eating_and_Drinking_at_Airport
ggplot(surveyDataRaw,aes(Eating_and_Drinking_at_Airport)) + geom_histogram(stat="count")
#Eating_and_Drinking_at_Airport
#ggplot(surveyDataRaw,aes(Eating_and_Drinking_at_Airport)) + geom_histogram(stat="count")
ggplot(surveyDataRaw,aes(y=Satisfaction,x=Eating_and_Drinking_at_Airport)) + geom_smooth(method="glm") + geom_count()
#Eating_and_Drinking_at_Airport
#ggplot(surveyDataRaw,aes(Eating_and_Drinking_at_Airport)) + geom_histogram(stat="count")
ggplot(surveyDataRaw,aes(Eating_and_Drinking_at_Airport)) + geom_boxplot()
#Class
ggplot(surveyDataRaw,aes(Class)) + geom_histogram(stat="count")
ggplot(surveyDataRaw,aes(y=Satisfaction,x=Class)) + geom_boxplot()
ggplot(surveyDataRaw,aes(y=Satisfaction,x=Day_of_Month)) + geom_boxplot()
ggplot(surveyDataRaw,aes(y=Satisfaction,x=Day_of_Month)) + geom_smooth(method="glm") + geom_count()
#Scheduled_Departure_Hour
ggplot(surveyDataRaw,aes(Scheduled_Departure_Hour)) + geom_histogram(stat="count")
ggplot(surveyDataRaw,aes(y=Satisfaction,x=Scheduled_Departure_Hour)) + geom_boxplot()
ggplot(surveyDataRaw,aes(y=Satisfaction,x=Scheduled_Departure_Hour)) + geom_smooth(method="glm") + geom_count()
#Scheduled_Departure_Hour
ggplot(surveyDataRaw,aes(Scheduled_Departure_Hour)) + geom_histogram(stat="count")
#Flight_time_in_minutes
ggplot(surveyDataRaw,aes(Flight_time_in_minutes)) + geom_histogram(stat="count")
ggplot(surveyDataRaw,aes(y=Satisfaction,x=Flight_time_in_minutes)) + geom_smooth(method="glm") + geom_count()
#Flight_time_in_minutes
ggplot(surveyDataRaw,aes(Flight_time_in_minutes)) + geom_histogram(stat="count")
#Age vs Others
#Age:Percent_of_Flight_with_other_Airlines
ggplot(surveyDataRaw,aes(x=Age,y=Percent_of_Flight_with_other_Airlines,color=Satisfaction)) + geom_smooth(color="red") + geom_count()
ggplot(surveyDataRaw[surveyDataRaw$Percent_of_Flight_with_other_Airlines<50,],aes(x=Age,y=Percent_of_Flight_with_other_Airlines,color=Satisfaction)) + geom_smooth(color="red") + geom_count()
#Age vs Others
#Age:Percent_of_Flight_with_other_Airlines
ggplot(surveyDataRaw,aes(x=Age,y=Percent_of_Flight_with_other_Airlines,color=Satisfaction)) + geom_smooth(color="red",method="glm") + geom_count()
#Age:No._of_other_Loyalty_Cards
ggplot(surveyDataRaw,aes(x=Age,y=No._of_other_Loyalty_Cards,color=Satisfaction)) + geom_smooth(color="red") + geom_count()
#Age:Eating_and_Drinking_at_Airport
ggplot(surveyDataRaw,aes(x=Age,y=Eating_and_Drinking_at_Airport,color=Satisfaction)) + geom_smooth(color="red") + geom_point()
#Age vs Satisfacation
ggplot(surveyDataRaw,aes(x=Age,y=Satisfaction)) + geom_smooth(color="red") + geom_count()
#Age vs Satisfacation
ggplot(surveyDataRaw,aes(x=Age,y=Satisfaction)) + geom_smooth(color="red",method="glm") + geom_count()
#No_of_Flights_p.a.:Departure_Delay_in_Minutes
ggplot(surveyDataRaw,aes(x=No_of_Flights_p.a.,y=Departure_Delay_in_Minutes,color=Satisfaction)) + geom_count()
#No_of_Flights_p.a.:Arrival_Delay_in_Minutes
ggplot(surveyDataRaw,aes(x=No_of_Flights_p.a.,y=Arrival_Delay_in_Minutes,color=Satisfaction)) + geom_count()
#No_of_Flights_p.a.:Flight_time_in_minutes
ggplot(surveyDataRaw,aes(x=No_of_Flights_p.a.,y=Flight_time_in_minutes,color=Satisfaction)) + geom_count()
#No_of_Flights_p.a.:Flight_Distance
ggplot(surveyDataRaw,aes(x=No_of_Flights_p.a.,y=Flight_Distance,color=Satisfaction)) + geom_count()
#Scheduled_Departure_Hour:Flight_time_in_minutes
ggplot(surveyDataRaw,aes(y=Scheduled_Departure_Hour,x=Flight_time_in_minutes,color=Satisfaction))  + geom_count()
#Shopping_Amount_at_Airport:ClassEco_Plus
ggplot(surveyDataRaw[surveyDataRaw$Class=="Eco Plus",],aes(x=Shopping_Amount_at_Airport,y=Satisfaction))  + geom_count() + ggtitle("Eco Plus")
#Type_of_TravelMileage_tickets:No._of_other_Loyalty_Cards
ggplot(surveyDataRaw[surveyDataRaw$Type_of_Travel=="Mileage tickets",],aes(x=No._of_other_Loyalty_Cards,y=Satisfaction))  + geom_count() + ggtitle("Mileage Tickets")
#Type_of_TravelPersonal_Travel:Scheduled_Departure_Hour
ggplot(surveyDataRaw[surveyDataRaw$Type_of_Travel=="Personal Travel",],aes(x=Scheduled_Departure_Hour,y=Satisfaction))  + geom_count() + ggtitle("Personal Travel")
#Type_of_TravelPersonal_Travel:Shopping_Amount_at_Airport
ggplot(surveyDataRaw[surveyDataRaw$Type_of_Travel=="Personal Travel",],aes(x=Shopping_Amount_at_Airport,y=Satisfaction))  + geom_count() + ggtitle("Personal Travel")
#Type_of_TravelMileage_tickets:No._of_other_Loyalty_Cards
ggplot(surveyDataRaw[surveyDataRaw$Type_of_Travel=="Mileage tickets",],aes(x=No._of_other_Loyalty_Cards,y=Satisfaction))  + geom_count() + ggtitle("Mileage Tickets") + geom_smooth()
#Type_of_TravelPersonal_Travel:Scheduled_Departure_Hour
ggplot(surveyDataRaw[surveyDataRaw$Type_of_Travel=="Personal Travel",],aes(x=Scheduled_Departure_Hour,y=Satisfaction))  + geom_count() + ggtitle("Personal Travel")+ geom_smooth()
#Type_of_TravelPersonal_Travel:Shopping_Amount_at_Airport
ggplot(surveyDataRaw[surveyDataRaw$Type_of_Travel=="Personal Travel",],aes(x=Shopping_Amount_at_Airport,y=Satisfaction))  + geom_count() + ggtitle("Personal Travel")+ geom_smooth()
#No_of_Flights_p.a. vs Age Flight_Distance - For arules  verification, business question 2
ggplot(surveyDataRaw,aes(y=No_of_Flights_p.a.,x=Age,color=Satisfaction)) + geom_smooth() + geom_count()
plot(nn_model)
plot(nn_model)
remove.packages("sqldf")
install.packages("sqldf")
install.packages("sparkR")
library(readxl)
library(lubridate)
library(dplyr)
library(zoo)
library(lmtest)
library(sqldf)
library(varhandle)
library(car)
library(caret)
library(tidyr)
library(tidyverse)    # data manipulation and visualization
library(kernlab)      # SVM methodology
library(e1071)
#library(RColorBrewer)
library(arules)
library(arulesViz)
#install.packages("factoextra")
library(factoextra)
#install.packages("factoextra")
library(factoextra)
#install.packages("factoextra")
library(factoextra)
#install.packages("factoextra")
library(factoextra)
#install.packages("factoextra")
library(factoextra)
library(tm)
#install.packages("tm")
library(stringr)
library(wordcloud)
# ONCE: install.packages("Snowball")
## NOTE Snowball is not yet available for R v 3.5.x
## So I cannot use it  - yet...
##library("Snowball")
##set working directory
## ONCE: install.packages("slam")
library(slam)
library(quanteda)
## ONCE: install.packages("quanteda")
## Note - this includes SnowballC
library(SnowballC)
library(arules)
## ONCE: install.packages("wordcloud")
library(wordcloud)
##ONCE: install.packages('proxy')
library(proxy)
library(cluster)
library(stringi)
library(proxy)
library(Matrix)
library(tidytext) # convert DTM to DF
library(plyr) ## for adply
library(ggplot2)
library(factoextra) # for fviz
library(mclust) # for Mclust EM clustering
#install.packages("slam")
library(slam)
#install.packages("tm")
library(tm)
#install.packages("factoextra")
library(factoextra)
#install.packages("factoextra")
library(factoextra)
#install.packages("factoextra")
library(factoextra)
#install.packages("factoextra")
library(factoextra)
library(tm)
#install.packages("tm")
library(stringr)
library(wordcloud)
# ONCE: install.packages("Snowball")
## NOTE Snowball is not yet available for R v 3.5.x
## So I cannot use it  - yet...
##library("Snowball")
##set working directory
## ONCE: install.packages("slam")
library(slam)
library(quanteda)
## ONCE: install.packages("quanteda")
## Note - this includes SnowballC
library(SnowballC)
library(arules)
## ONCE: install.packages("wordcloud")
library(wordcloud)
##ONCE: install.packages('proxy')
library(proxy)
library(cluster)
library(stringi)
library(proxy)
library(Matrix)
library(tidytext) # convert DTM to DF
library(plyr) ## for adply
library(ggplot2)
library(factoextra) # for fviz
library(mclust) # for Mclust EM clustering
#install.packages("slam")
library(slam)
#install.packages("tm")
library(tm)
#install.packages("factoextra")
library(factoextra)
install.packages("mclust")
library(tm)
#install.packages("tm")
library(stringr)
library(wordcloud)
# ONCE: install.packages("Snowball")
## NOTE Snowball is not yet available for R v 3.5.x
## So I cannot use it  - yet...
##library("Snowball")
##set working directory
## ONCE: install.packages("slam")
library(slam)
library(quanteda)
## ONCE: install.packages("quanteda")
## Note - this includes SnowballC
library(SnowballC)
library(arules)
## ONCE: install.packages("wordcloud")
library(wordcloud)
##ONCE: install.packages('proxy')
library(proxy)
library(cluster)
library(stringi)
library(proxy)
library(Matrix)
library(tidytext) # convert DTM to DF
library(plyr) ## for adply
library(ggplot2)
library(factoextra) # for fviz
library(mclust) # for Mclust EM clustering
#install.packages("slam")
library(slam)
#install.packages("tm")
library(tm)
#install.packages("factoextra")
library(factoextra)
library(tm)
#install.packages("tm")
library(stringr)
library(wordcloud)
# ONCE: install.packages("Snowball")
## NOTE Snowball is not yet available for R v 3.5.x
## So I cannot use it  - yet...
##library("Snowball")
##set working directory
## ONCE: install.packages("slam")
library(slam)
library(quanteda)
## ONCE: install.packages("quanteda")
## Note - this includes SnowballC
library(SnowballC)
library(arules)
## ONCE: install.packages("wordcloud")
library(wordcloud)
##ONCE: install.packages('proxy')
library(proxy)
library(cluster)
library(stringi)
library(proxy)
library(Matrix)
library(tidytext) # convert DTM to DF
library(plyr) ## for adply
library(ggplot2)
#library(factoextra) # for fviz
library(mclust) # for Mclust EM clustering
#install.packages("slam")
library(slam)
#install.packages("tm")
library(tm)
#install.packages("factoextra")
library(factoextra)
library(tm)
#install.packages("tm")
library(stringr)
library(wordcloud)
# ONCE: install.packages("Snowball")
## NOTE Snowball is not yet available for R v 3.5.x
## So I cannot use it  - yet...
##library("Snowball")
##set working directory
## ONCE: install.packages("slam")
library(slam)
library(quanteda)
## ONCE: install.packages("quanteda")
## Note - this includes SnowballC
library(SnowballC)
library(arules)
## ONCE: install.packages("wordcloud")
library(wordcloud)
##ONCE: install.packages('proxy')
#library(proxy)
library(cluster)
library(stringi)
library(proxy)
library(Matrix)
library(tidytext) # convert DTM to DF
library(plyr) ## for adply
library(ggplot2)
#library(factoextra) # for fviz
library(mclust) # for Mclust EM clustering
#install.packages("slam")
library(slam)
#install.packages("tm")
library(tm)
#install.packages("factoextra")
library(factoextra)
library(tm)
library(stringr)
library(wordcloud)
library(cluster)
library(stringi)
library(Matrix)
library(tidytext) # convert DTM to DF
library(plyr) ## for adply
library(ggplot2)
library(SnowballC)
library(mclust) # for Mclust EM clustering
library(slam)
library(factoextra)
library(philentropy)
setwd("/Users/tamtam/Dropbox/Masters/s2 - Winter 2021/IST_707_Data_Analytics/HW4")
novelCorpus <- Corpus(DirSource("FedPapersCorpus"))
#str(novelCorpus)
names<- as.list(names(novelCorpus))
fixed_names = vector(mode = "list", length = length(names))
for (i in 1:length(names)) {
names[[i]]<-str_split(names[[i]],"_")[[1]]
fixed_names[[i]]<-names[[i]][1]
#print(fixed_names[[i]])
#novelCorpus[[i]]$meta$author<-fixed_names[[i]]
#meta(novelCorpus[[i]],5)<-fixed_names[[i]]
}
#meta(novelCorpus,tag="author",type="corpus") <- unlist(fixed_names, use.names=FALSE)
meta(novelCorpus[[1]])
meta(novelCorpus[[1]],5)
#novelsDF <- readtext("FedPapersCorpus/*.txt",
#  docvarsfrom = "filenames",
#  dvsep = "_",
#  encoding = "UTF-8"
#)
#novelCorpus <- corpus(novelsDF)
#summary(novelCorpus)
#str(novelCorpus)
#(getTransformations())
ndocs<-length(novelCorpus)
summary(novelCorpus)
meta(novelCorpus[[1]])
meta(novelCorpus[[1]],5)
# ignore extremely rare words i.e. terms that appear in less then 1% of the documents
minTermFreq <- ndocs * 0.0001
# ignore overly common words i.e. terms that appear in more than 50% of the documents
maxTermFreq <- ndocs * 1
stopWords <-stopwords('english')
dtm <- DocumentTermMatrix(novelCorpus,
control = list(
stopwords = TRUE,
wordLengths=c(3, 15),
removePunctuation = T,
removeNumbers = T,
tolower=T,
stemming = T,
remove_separators = T,
stopwords = stopWords,
#removeWords(STOPS),
#removeWords(MyStopwords),
bounds = list(global = c(minTermFreq, maxTermFreq))
))
#Fix the document Label, so we get a nice graph label
dtm$dimnames$Docs<-unlist(fixed_names, use.names=FALSE)
dtmMatrix <- as.matrix(dtm)
dtmMatrix[1:13,1:5]
normalizeDTM<-function(dtmMatrix){
dtmMatrix_1 <- apply(dtmMatrix, 1, function(i) round(i/sum(i),3))
## transpose
dtmMatrix_norm <- t(dtmMatrix_1)
return(dtmMatrix_norm)
}
dtmMatrix_norm<-normalizeDTM(dtm)
wordcloud(colnames(dtmMatrix), dtmMatrix[13,], max.words = 70)
(head(sort(as.matrix(dtm)[13,], decreasing = TRUE), n=20))
############## Distance Measures ######################
getDistMethods()
m  <- dtmMatrix
m_norm <- dtmMatrix_norm
# # # m <- m[1:2, 1:3]
distMatrix_E <- dist(m, method="euclidean")
print(distMatrix_E)
distMatrix_C <- dist(m, method="cosine")
distMatrix_C <- distance(m, method="cosine")
print(distMatrix_C)
# # # m <- m[1:2, 1:3]
distMatrix_E <- distance(m, method="euclidean")
print(distMatrix_E)
distMatrix_C <- distance(m, method="cosine")
print(distMatrix_C)
distMatrix_C_norm <- distance(m_norm, method="cosine")
## Euclidean
groups_E <- hclust(distMatrix_E,method="ward.D")
plot(groups_E, cex=0.9, hang=-1)
# # # m <- m[1:2, 1:3]
distMatrix_E <- dist(m, method="euclidean")
print(distMatrix_E)
distMatrix_C <- dist(m, method="cosine")
library(proxy)
# # # m <- m[1:2, 1:3]
distMatrix_E <- dist(m, method="euclidean")
print(distMatrix_E)
distMatrix_C <- dist(m, method="cosine")
print(distMatrix_C)
distMatrix_C_norm <- dist(m_norm, method="cosine")
## Euclidean
groups_E <- hclust(distMatrix_E,method="ward.D")
plot(groups_E, cex=0.9, hang=-1)
rect.hclust(groups_E, k=4)
## Cosine Similarity
groups_C <- hclust(distMatrix_C,method="ward.D")
plot(groups_C, cex=0.9, hang=-1)
rect.hclust(groups_C, k=4)
## Cosine Similarity for Normalized Matrix
groups_C_n <- hclust(distMatrix_C_norm,method="ward.D")
plot(groups_C_n, cex=0.9, hang=-1)
rect.hclust(groups_C_n, k=4)
####################   k means clustering -----------------------------
X <- m_norm
## Remember that kmeans uses a matrix of ONLY NUMBERS
## We have this so we are OK.
## Manhattan gives the best vis results!
distance1 <- get_dist(X,method = "manhattan")
fviz_dist(distance1, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
distance2 <- get_dist(X,method = "pearson")
fviz_dist(distance2, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
distance3 <- get_dist(X,method = "canberra")
fviz_dist(distance3, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
distance4 <- get_dist(X,method = "spearman")
fviz_dist(distance4, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
X <- t(X)
## Now scale the data
#X <- scale(X)
str(X)
## k means
kmeansFIT_1 <- kmeans(X,centers=4)
#(kmeansFIT1)
summary(kmeansFIT_1)
#(kmeansFIT_1$cluster)
fviz_cluster(kmeansFIT_1, data = X)
ClusFI <- Mclust(X,G=6)
ClusFI <- Mclust(X,G=6)
View(X)
X<-m
X <- t(X)
ClusFI <- Mclust(X,G=6)
summary(ClusFI)
plot(ClusFI, what = "classification")
